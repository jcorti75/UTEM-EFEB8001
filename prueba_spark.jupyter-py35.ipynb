{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession(sc).builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"jdbc\").option(\"url\",\"jdbc:sqlserver://10.38.96.84:1433;databaseName=ANALISIS_BI\").option(\"dbtable\", \"dbo.JCB_NODO\").option(\"user\",\"Usr_watson\").option(\"password\", \"usrwatson1\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  NODO|HPS|\n",
      "+------+---+\n",
      "|BUIC01|371|\n",
      "|BUIC02|361|\n",
      "|BUIC03|517|\n",
      "|BUIC04|539|\n",
      "|BUIC05|432|\n",
      "|CCHC01|441|\n",
      "|CCHC02|285|\n",
      "|CCHC03|303|\n",
      "|CCHC04|445|\n",
      "|CRRC17|441|\n",
      "|CRRC18|215|\n",
      "|CRRC19|434|\n",
      "|CRRC20|496|\n",
      "|CRRC21|249|\n",
      "|CRRC22|385|\n",
      "|HRBC01|482|\n",
      "|HRBC02|386|\n",
      "|HRBC04|  5|\n",
      "|HRBC05| 20|\n",
      "|HRBC06|416|\n",
      "+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NODO: string (nullable = true)\n",
      " |-- HPS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NODO', 'string'), ('HPS', 'string')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODO', 'HPS']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.dropDuplicates(subset=[\"NODO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        NODO|\n",
      "+------------+\n",
      "|      BUIC04|\n",
      "|      SBEC08|\n",
      "|      NUNC06|\n",
      "|      PHUC15|\n",
      "|      RENC03|\n",
      "|      PHUC16|\n",
      "|      BUIC02|\n",
      "|      MAIC06|\n",
      "|      CCHC01|\n",
      "|      PFLC27|\n",
      "|SBEC08 lib.1|\n",
      "|      PALC05|\n",
      "|      RENC01|\n",
      "| SBEC01 ext.|\n",
      "|      PHUC10|\n",
      "|      PUDC06|\n",
      "|      PHUC21|\n",
      "|      SBEC06|\n",
      "|      NUNC05|\n",
      "|      LIMC01|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"NODO\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|(HPS >= 300)|\n",
      "+------------+\n",
      "|        true|\n",
      "|        true|\n",
      "|        true|\n",
      "|        true|\n",
      "|        true|\n",
      "|       false|\n",
      "|        true|\n",
      "|       false|\n",
      "|        true|\n",
      "|        true|\n",
      "|        null|\n",
      "|        true|\n",
      "|        true|\n",
      "|        null|\n",
      "|        true|\n",
      "|       false|\n",
      "|       false|\n",
      "|        true|\n",
      "|       false|\n",
      "|        true|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"HPS\"]>=300).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|condicion|\n",
      "+---------+\n",
      "|        1|\n",
      "|        1|\n",
      "|        1|\n",
      "|        1|\n",
      "|        1|\n",
      "|        0|\n",
      "|        1|\n",
      "|        0|\n",
      "|        1|\n",
      "|        1|\n",
      "|        0|\n",
      "|        1|\n",
      "|        1|\n",
      "|        0|\n",
      "|        1|\n",
      "|        0|\n",
      "|        0|\n",
      "|        1|\n",
      "|        0|\n",
      "|        1|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.select(F.when(F.col(\"HPS\")>=300,1).otherwise(0).alias(\"condicion\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(F.col(\"HPS\")>=300).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+\n",
      "|        NODO|HPS|HPS_x2|\n",
      "+------------+---+------+\n",
      "|      BUIC04|539|1078.0|\n",
      "|      SBEC08|467| 934.0|\n",
      "|      NUNC06|432| 864.0|\n",
      "|      PHUC15|502|1004.0|\n",
      "|      RENC03|458| 916.0|\n",
      "|      PHUC16|250| 500.0|\n",
      "|      BUIC02|361| 722.0|\n",
      "|      MAIC06|286| 572.0|\n",
      "|      CCHC01|441| 882.0|\n",
      "|      PFLC27|364| 728.0|\n",
      "|SBEC08 lib.1|   |  null|\n",
      "|      PALC05|578|1156.0|\n",
      "|      RENC01|488| 976.0|\n",
      "| SBEC01 ext.|   |  null|\n",
      "|      PHUC10|381| 762.0|\n",
      "|      PUDC06|170| 340.0|\n",
      "|      PHUC21| 67| 134.0|\n",
      "|      SBEC06|486| 972.0|\n",
      "|      NUNC05|275| 550.0|\n",
      "|      LIMC01|480| 960.0|\n",
      "+------------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"HPS_x2\", F.col(\"HPS\")*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NODO: string (nullable = true)\n",
      " |-- nombre_nuevo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"HPS\", \"nombre_nuevo\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+\n",
      "|summary|  NODO|               HPS|\n",
      "+-------+------+------------------+\n",
      "|  count|   174|               174|\n",
      "|   mean|  null| 350.7083333333333|\n",
      "| stddev|  null|140.55558510777868|\n",
      "|    min|BUIC01|                  |\n",
      "|    max|SBEC28|                99|\n",
      "+-------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df.describe().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        NODO|\n",
      "+------------+\n",
      "|      BUIC04|\n",
      "|      SBEC08|\n",
      "|      NUNC06|\n",
      "|      PHUC15|\n",
      "|      RENC03|\n",
      "|      PHUC16|\n",
      "|      BUIC02|\n",
      "|      MAIC06|\n",
      "|      CCHC01|\n",
      "|      PFLC27|\n",
      "|SBEC08 lib.1|\n",
      "|      PALC05|\n",
      "|      RENC01|\n",
      "| SBEC01 ext.|\n",
      "|      PHUC10|\n",
      "|      PUDC06|\n",
      "|      PHUC21|\n",
      "|      SBEC06|\n",
      "|      NUNC05|\n",
      "|      LIMC01|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"NODO\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"select NODO, count(*) as cuenta from dataframe where HPS>=300 group by NODO order by cuenta desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  NODO|cuenta|\n",
      "+------+------+\n",
      "|BUIC04|     1|\n",
      "|SBEC08|     1|\n",
      "|NUNC06|     1|\n",
      "|PHUC15|     1|\n",
      "|RENC03|     1|\n",
      "|BUIC02|     1|\n",
      "|CCHC01|     1|\n",
      "|PFLC27|     1|\n",
      "|PALC05|     1|\n",
      "|RENC01|     1|\n",
      "|PHUC10|     1|\n",
      "|SBEC06|     1|\n",
      "|LIMC01|     1|\n",
      "|MAIC12|     1|\n",
      "|PALC07|     1|\n",
      "|PFLC03|     1|\n",
      "|PHUC04|     1|\n",
      "|PUDC01|     1|\n",
      "|PFLC13|     1|\n",
      "|HRBC07|     1|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.col(\"HPS\")>=300).groupBy(\"NODO\").count().withColumnRenamed(\"count\",\"cuenta\").orderBy(\"cuenta\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        NODO|\n",
      "+------------+\n",
      "|      BUIC04|\n",
      "|      SBEC08|\n",
      "|      NUNC06|\n",
      "|      PHUC15|\n",
      "|      RENC03|\n",
      "|      PHUC16|\n",
      "|      BUIC02|\n",
      "|      MAIC06|\n",
      "|      CCHC01|\n",
      "|      PFLC27|\n",
      "|SBEC08 lib.1|\n",
      "|      PALC05|\n",
      "|      RENC01|\n",
      "| SBEC01 ext.|\n",
      "|      PHUC10|\n",
      "|      PUDC06|\n",
      "|      PHUC21|\n",
      "|      SBEC06|\n",
      "|      NUNC05|\n",
      "|      LIMC01|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"HPS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=df.filter(F.col(\"HPS\")>=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Filter (isnotnull(HPS#34) && (cast(HPS#34 as int) >= 300))\n",
      "+- SortAggregate(key=[NODO#33], functions=[first(HPS#34, false)])\n",
      "   +- *Sort [NODO#33 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(NODO#33, 200)\n",
      "         +- SortAggregate(key=[NODO#33], functions=[partial_first(HPS#34, false)])\n",
      "            +- *Sort [NODO#33 ASC NULLS FIRST], false, 0\n",
      "               +- *Scan JDBCRelation(dbo.JCB_NODO) [numPartitions=1] [NODO#33,HPS#34] ReadSchema: struct<NODO:string,HPS:string>\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.createTempView(\"mi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-59-1f7fd3745ba6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-1f7fd3745ba6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    select * from mi_data where HPS>=300\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  NODO|HPS|\n",
      "+------+---+\n",
      "|BUIC01|371|\n",
      "|BUIC02|361|\n",
      "|BUIC03|517|\n",
      "|BUIC04|539|\n",
      "|BUIC05|432|\n",
      "|CCHC01|441|\n",
      "|CCHC03|303|\n",
      "|CCHC04|445|\n",
      "|CRRC17|441|\n",
      "|CRRC19|434|\n",
      "|CRRC20|496|\n",
      "|CRRC22|385|\n",
      "|HRBC01|482|\n",
      "|HRBC02|386|\n",
      "|HRBC06|416|\n",
      "|HRBC07|386|\n",
      "|HRBC08|476|\n",
      "|LIMC01|480|\n",
      "|LIMC02|380|\n",
      "|LIMC04|531|\n",
      "+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from mi_data where HPS>=300\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"jdbc\").option(\"url\",\"jdbc:sqlserver://10.38.96.84:1433;databaseName=ANALISIS_BI\").option(\"dbtable\", \"(select NODO from dbo.JCB_NODO where HPS>=300) TBL\").option(\"user\",\"Usr_watson\").option(\"password\", \"usrwatson1\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  NODO|\n",
      "+------+\n",
      "|BUIC01|\n",
      "|BUIC02|\n",
      "|BUIC03|\n",
      "|BUIC04|\n",
      "|BUIC05|\n",
      "|CCHC01|\n",
      "|CCHC03|\n",
      "|CCHC04|\n",
      "|CRRC17|\n",
      "|CRRC19|\n",
      "|CRRC20|\n",
      "|CRRC22|\n",
      "|HRBC01|\n",
      "|HRBC02|\n",
      "|HRBC06|\n",
      "|HRBC07|\n",
      "|HRBC08|\n",
      "|LIMC01|\n",
      "|LIMC02|\n",
      "|LIMC04|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NODO: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as T\n",
    "df.withColumn(\"NODO\", F.col(\"NODO\").cast(T.StringType())).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.ml.feature as MLF\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "spark = SparkSession(sc).builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"jdbc\").option(\"url\",\"jdbc:sqlserver://10.38.96.84:1433;databaseName=ANALISIS_BI\").option(\"dbtable\", \"dbo.JCB_INTERNET_TV_HFC_12M\").option(\"user\",\"Usr_watson\").option(\"password\", \"usrwatson1\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numericas=[\"PROM_PAGO_12M\",\"FLAG_UPSELLING\", \"ANTIGUEDAD_HFC\"]\n",
    "categoricas=[\"TIPO_ENTRADA\", \"CANAL_VTA\"]\n",
    "target=\"FLAG_TV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df.select(numericas+categoricas+[target])\n",
    "df=df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187335"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler_numerical = MLF.VectorAssembler(inputCols = numericas , outputCol = \"assembled_numerical\")\n",
    "normalizer = MLF.StandardScaler(inputCol=\"assembled_numerical\", outputCol=\"normalized_features\")\n",
    "indexers = [MLF.StringIndexer(inputCol=c, outputCol=\"{0}_index\".format(c)) for c in categoricas]\n",
    "encoders = [MLF.OneHotEncoder(inputCol=indexer.getOutputCol(),outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers]\n",
    "assembler = MLF.VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]+[\"normalized_features\"], outputCol=\"outputFeatures\")\n",
    "pipeline = Pipeline(stages=[assembler_numerical,normalizer]+indexers+encoders+[assembler])\n",
    "preprocessing = pipeline.fit(df)\n",
    "df_transformed=preprocessing.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train, df_test= df_transformed.randomSplit([0.7, 0.3], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131142"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train=df_train.union(df_train.filter(F.col(target)==1).sample(withReplacement=True,fraction=30.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278044"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|FLAG_TV| count|\n",
      "+-------+------+\n",
      "|      1|151799|\n",
      "|      0|126245|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56193"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"outputFeatures\", labelCol=target)\n",
    "lrModel = lr.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicciones=lrModel.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|47721|\n",
      "|       1.0| 8472|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicciones.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/mllib/evaluation.py:262: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
     ]
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(predicciones.withColumn(target, F.col(target).cast(T.DoubleType())).select(\"prediction\",target).rdd)\n",
    "recall_metrics = round(metrics.recall(1),4)\n",
    "accuracy_metrics = round(metrics.accuracy,4)\n",
    "fmeasure_metrics = round(metrics.fMeasure(),4)\n",
    "precision_metrics = round(metrics.precision(1),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|recall    : 0.87|\n",
      "+------------------+\n",
      "|accuracy  : 0.8779|\n",
      "+------------------+\n",
      "|fMeasure  : 0.8779|\n",
      "+------------------+\n",
      "|precision : 0.2236|\n",
      "+------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"+------------------+\")\n",
    "print(\"|recall    : %s|\"%recall_metrics)\n",
    "print(\"+------------------+\")\n",
    "print(\"|accuracy  : %s|\"%accuracy_metrics)\n",
    "print(\"+------------------+\")\n",
    "print(\"|fMeasure  : %s|\"%fmeasure_metrics)\n",
    "print(\"+------------------+\")\n",
    "print(\"|precision : %s|\"%precision_metrics)\n",
    "print(\"+------------------+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicciones_pd=predicciones.select([target, \"prediction\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93     54016\n",
      "           1       0.22      0.87      0.36      2177\n",
      "\n",
      "    accuracy                           0.88     56193\n",
      "   macro avg       0.61      0.87      0.64     56193\n",
      "weighted avg       0.96      0.88      0.91     56193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predicciones_pd[target], predicciones_pd[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
